Stats 155 Class Notes 2012-11-14
================================

```{r name="setup", child="notes-setup.Rmd"}
# boilerplate
```


### mLM and reading R^2 as more terms change

Show how terms increase R^2, which gives the fraction of distance to the response variable.

Introduce the idea of the log to handle the interaction between sector and sex, why the dollar amount of difference between the sectors should differ between the sexes.


### Two categorical variables

Example: Sector and sex

```{r}
mod = lm( wage ~ sector*sex, data=CPS85)
anova(mod)
```
Have we detected an interaction?

#### ACTIVITY

Create the set of nested models and construct the ANOVA table by hand.

#### Digression: Log prices and wages

Prices are relative.  An indication of this is the almost universal use of percentages to describe inflation, wage increases, etc.  For example, an often quoted number is that women earn approximately 72 cents for each dollar earned by a man.  

The naive way to find this number, which is in fact the way it is found, is to divide the average wage of women by the average wage of men, e.g.
```{r}
mean( wage ~ sex, data=CPS85)
7.88/9.99
```
Close to the quoted number in this data set from the 1980s.

A better way is to work with log wages, find the contribution from sex, and then convert that back into a multiplier.  That let's us adjust for various other factors.  Here's the basic calculation, done in log-wage style:
```{r}
lm( log(wage) ~ sex, data=CPS85 )
exp(-0.2312)
```

Now we can include covariates:
```{r}
lm( log(wage) ~ sex + sector + exper + educ, data=CPS85 )
exp(-0.2197)
```

Hardly any difference.  But maybe the model should be more complicated. 

#### Activity

Use ANOVA and log wages to see if interactions should be included in the model.  Example:
```{r}
mod = lm(log(wage) ~ sector*sex, data=CPS85)
anova(mod)
```

####  Student Activity
Question: Is there an interaction between age and mileage in the used car data?  Does it show up if we look at log prices?
```{r}
cars = fetchData("used-hondas.csv")
anova(lm(log(Price)~Age*Mileage, data=cars))
```


### Example: Do professors vary in how they grade? Revisited

One place where F shines is when we want to look at many explanatory vectors collectively.

In the last class, we looked at professor-wise gradepoint averages, with an eye to the question of whether some professors are easy grading.  We used as a test statistic, the model coefficient for each professor, and ran into the question of multiple comparisons.  

Now let's return to the question using analysis of variance.

```{r}
grades = fetchData("grades.csv")
courses = fetchData("courses.csv")
g2n = fetchData("grade-to-number.csv")
all = merge( grades, courses)
all = merge(all, g2n) # a data set of every grade given, etc.
```

Suppose, instead of being concerned about individual professors, we were interested in the professorate as a whole: do they grade in a consistent way, where "consistent" means, "draw grades from a common pool."  This test can be done easily.  Build the model and see if the explanatory variable accounts for more than is likely to arise from chance:
```{r}
mod1 = lm( gradepoint ~ iid, data=all )
r.squared(mod1)
```

The regression report actually gives a p-value for this r.squared.  It's not any different than we would get by travelling to Planet Null:  randomizing `iid` and seeing what is the distribution of R^2 on Planet Null.

Another way to summarize the model is with an ANOVA report:
```{r}
anova(mod1)
```


### Theory of F

Explain R^2 in terms of the graph of (hypothetical) R^2 versus number of junky model vectors.  F is the ratio of segment slopes

Now do it stepwise by finding the sum of squares of the fitted model values in a set of models `~1` and `~1 + iid`
* Sum of squares of the fitted model values
* DF ("degrees of freedom") a count of coefficients
* Residual SS --- sum of squares of the residuals
* Residual DF --- $n$ - DF of the model
* Look at how things change when going from `~1` to `~1 +iid`.
* "Mean square" is SS/DF

A way to think of the F statistic: miles per gallon for the model terms compared to miles per gallon for the 

### Breaking up the Variance into Parts

Of course, it's not fair to credit professors for variation in grades that is really due to the students.  So we want to divide up the variation into that due to the students and that due to the professors.  ANOVA let's you do this:
```{r}
mod2 = lm( gradepoint ~ sid + iid, data=all )
anova(mod2)
```

Interestingly, the result depends on the order in which you put the model terms, even though the model values do not at all depend on this.
```{r}
mod3 = lm( gradepoint ~ iid + sid, data=all )
anova(mod3)
sum( (fitted(mod2) - fitted(mod3))^2) # model values are the same
```

Eating Up the Variance
----------------

The F statistic compares the "credit" earned by a model term to the mean square residual, which can be interpreted as the credit that would be earned by a junky random term.

Fit a model and add in some random terms.  Show that the F for the random terms is about 1 and that the mean square of the residual is hardly changed by the random terms.

```{r}
mod0 = lm(wage ~ sector + sex + exper, data=CPS85)
anova(mod0)
```
The mean square residual is about 20.

Now throw in some junk:
```{r}
mod10 = lm( wage ~ sector + sex + exper + rand(10), data=CPS85)
anova(mod10)
```

But what if a term eats more variance than a junky term.  That term makes it easier for the other terms to show significance.

EXAMPLE:  Difference in age between husband and wife in couples getting married.

Ask: Who is older in a married couple, the man or the woman?  By how much?  

Let's see if the data support this:
```{r}
m = fetchData("marriage.csv")
mod0 = mm( Age ~ Person, data=m )
mod0
confint(mod0)
mod1 = lm( Age ~ Person, data=m )
summary(mod1)
```
The point estimate is about right, but the margin of error is so large that we can't take this estimate very seriously.  The p-value is so large that we can't reject the null that there is no relationship between `Person` and age.

Try adding in some other variables, astrological sign, years of education, etc. and show that this doesn't help much.

Finally, add in the `BookpageID` variable.  
```{r}
mod2 = lm( Age ~ Person + BookpageID, data=m )
anova(mod2)
head(confint(mod2))
```

This gives an individual ID to each marriage.  Putting this in the model effectively holds the couple constant when considering the `Person`.  In terms of ANOVA, `BookpageID` is eating up lots and lots of variance.

### Example: Heating and Electricity Use

In theory, the electricity used in a house for lighting, computers, etc. should also be heating the house; eventually the energy in the electricity is transformed into heat.  But how much?  Perhaps the amount of electricity is small compared to the amount of heat.

Let's look just at winter months, November through March, when heating is used pretty steadily:
```{r}
utils = fetchData("utilities.csv")
winter = subset(utils, temp < 55 & ccf > 45 & month != 10 )
bwplot( gasbill/elecbill ~ as.factor(month), data=winter)
```
It looks like roughly twice as much is paid for gas as electricity.  So there's more money going into heating than lighting and other electricity uses. Considering that electricity is a much more expensive form of energy than natural gas, this suggests that there is much more energy being used for heating than for lighting.

But since energy is a physical quantity, there is a very good theory of it.  In fact, both kWh and ccf are forms of energy, and so there is a conversion between them.  (Actually, ccf is a volume, but since the volumetric energy density of natural gas varies only by a few percent depending on other factors, we can treat it approximately as an energy.)  Looking it up on in [Wikipedia](http://en.wikipedia.org/wiki/Therm) indicates that a ccf is roughly 29.3 kWh.

```{r}
bwplot( kwh/(29.3*ccf) ~ as.factor(month), data=winter)
```

Electricity energy use is about 10-20% compared to gas energy use in the winter months.  This suggests that we should be able to detect the electricity use.  It should show up as a negative coefficient on `kwh` in a model of `ccf`, with a magnitude of about $1/29.3$ (the conversion factor between kwh and ccf).
```{r}
mod1 = lm( ccf ~ kwh, data=winter )
summary( mod1 )
anova( mod1 )
```
Alas, we can't reject the null.  Note that the two reports are giving the same information. There is only explanatory vector and so there is no "team" effect for ANOVA to help us understand.

But there are other factors that affect natural gas use, and we may be able to explain some of these.  Doing so will reduce the size of the residuals and may make it easier to see the effect of electricity use on natural gas use.
```{r}
mod2 = lm( ccf ~ kwh + temp, data=winter )
summary(mod2)
anova(mod2)
```
When put first in the model, `kwh` shows up as significant in ANOVA.  But when put last (try it!), it doesn't.  That's because both `temp` and `kwh` have a common contributing cause: winter.  `temp` is capable of explaining some of `kwh` and so putting `temp` first grabs some of the credit.

### A sample-size calculation: One way to make Planet Alt.

How much data would we need to have to be able to test the conversion between `ccf` and `kwh` to within about 5%?  

Creating Planet Alt as a copy of Planet Null.  Resample it with various sample sizes until the confidence intervals get acceptable in size.

Emphasize: this cannot replace collecting the data, but indicates to us how much data we would need to collect to be able to get the sort of result we expect.

Choosing Model Terms
--------------------

Put terms first: they get credit.  Put terms later: they can help the earlier terms to get credit.

Look at sex and race in the wage data.  Put them first.  Put them last.  What are the different interpretations?

```{r}
mod0 = lm( log(wage) ~ race + sex, data=CPS85)
anova(mod0)
```
Race isn't showing up as significant, but sex is.  Perhaps if we eat up the variance by putting in more explanatory variables ...

```{r}
mod1 = lm( log(wage) ~ race + sex + sector + educ + exper + south, data=CPS85)
anova(mod1)
```
Significance of sex is even stronger  Race is right on the margin.  With more data (and this is a very limited amount of data) we might see something.

But what if someone argues that it's really educational levels etc that matter and you have to see how these effect wage first:
```{r}
mod3 = lm( log(wage) ~ sector + educ + exper + south + sex + race, data=CPS85)
anova(mod3)
```
No indication that race is playing a role.

QUESTION: But is race affecting education, experience, and sector?  If so, then race should go first.

An Odd Situation
-----------------

Sometimes the later term gets credit, but only because it was set up by an earlier term.  Without the earlier term, it wouldn't get credit.

Examples: 
* temperature in the alder data
* birth year, death year explaining lifespan.

ANOVA and near redundancy
------------------

The CPS85 data.  With ANOVA, you can throw all sorts of stuff into the model and not have to worry about redundancy or near redundancy. 

```{r}
m1 = lm( wage ~ sector + exper + age + educ, data=CPS85)
summary(m1) # terrible standard errors
anova(m1)
```


Still working ...

One of the advantages of Planet Null is that it's easy to get there and collect data.  So we can know a lot about it.  Discoveries about Planet Null have played a large role in statistics.

### Time for t

Let's collect lots of coefficients on Planet Null and see what they have in common.  We'll do this by randomizing the response variable:
```{r eval=FALSE,cache=TRUE}
mod1 = lm( shuffle(wage)-mean(wage) ~ shuffle(sector)*shuffle(sex)*shuffle(age)*shuffle(exper)*shuffle(married)*shuffle(union), data=CPS85)
houses = fetchData("SaratogaHouses.csv")
mod2 = lm( shuffle(Price)-mean(Price) ~ shuffle(Living.Area)*shuffle(Baths)*shuffle(Bedrooms)*shuffle(Fireplace)*shuffle(Acres), data=houses )
grades = fetchData("grades.csv")
g2n = fetchData("grade-to-number.csv")
courses = fetchData("courses.csv")
all = merge(grades, g2n)
all = merge(courses, all)
mod3 = lm( rand(1) ~ shuffle(sessionID) + shuffle(dept) + shuffle(sid), data=all)
```
