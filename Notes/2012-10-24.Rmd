Stats 155 Class Notes 2012-10-24
================================

```{r name="setup", child="notes-setup.Rmd"}
# boilerplate
```

Background
----------

All through the semester we have worked within a certain framework for modeling: models which partition variation in a response variable into an "explained" part (using the variation in explanatory variables) and into a "residual" part, which has been treated as completely random.

Today, we're going to work with a completely different framework for modeling, one that is completely descriptive and doesn't try to partition variation into deterministic and random parts --- everything is regarded as random.

Even when something is regarded as random, it doesn't mean that it is completely unpredictable.  For instance, the roll of a pair of dice is much more likely to result in a 7 than an 11.  The models we'll work with today are ways of describing probabilities of different outcome.


Definitions
-----------

A **probability** is a number between 0 and 1.  0 means "impossible" and 1 means "certain".  Values between 0 and 1 indicate possibility, with bigger numbers indicating a greater possibility.

A **random variable** is a quantity (a number) that is random.

A **sample space** (poorly named) is the set of possible values for that number.

A **probability model** is an assignment of a probability to each member of the sample space.

It's helpful to distinguish between two kinds of sample spaces that apply to random variables:

* **discrete numbers** such as the outcome of rolling a die
* **continuous numbers** that can take on any value in a range.  (The range might be infinite or finite.)

For discrete numbers, it's possible to assign a probability to each outcome.

For continuous numbers, it's possible to assign a probability to a range of outcomes.  Or, by dividing the probability by the extent of the range, one can assign a **probability density** to each outcome.  We usually treat this probability density as a function of the value of the random value: $p(x)$ 

We'll often use probabilities and probability densities in a similar way.  

* For a discrete sample space, the assigned probabilities over all the members of the space must add up to 1.
* For a continuous sample space, the integral over the assigned probability over the possible values must be one, that is: $\int_{-\infty}^{infty} p(x) dx = 1$.


Creating Probability Models
-------------------

There are several approaches to creating probability models, that is, to making an association between a probability and a member of the sample space:

* Indifference.  Or, in fancier words, equipartition. The 6 outcomes of a die roll are equally likely.  The two outcomes of a coin toss (represented as 0 or 1) are equally likely.
* Combination.  A given outcome can sometimes be described as a combination of outcomes from one or more trials: a compound event.  Using rules for the combination of probabilities, the probability of a compound event can be calculated from the known (or presumed) probabilities of the simpler events.
* Intuition or subjective knowledge.  "I think there's about a 1/3 chance I'll get an interview."
* Mechanism or setting.  There are a variety of probability models that are used for different settings.  We're going to study these.

Your job is to learn how the setting relates to the choice of probability model and the meaning of the parameter(s) for each model.

Some Important Probability Models
--------------------

Introduce the rxxx() operation for each.  For equal probabilities, just use 
`resample(1:k, size=n)`.  Generate random numbers from each.  Ask them to find the mean and standard deviation of each.

### Discrete

* Equal probabilities: e.g. die toss, coin flip.  Parameter: how many possibilities.
* Binomial: trials of a coin flip where the outcome is the number of "successes" or "heads" or "1s".  Parameters: 
    * **size** number of trials
    * **prob** probability of head on each trial
* Poisson: Number of events that happen in a given.  Example: number of cars passing by a point, number of shooting stars in a minute's observation of the sky, number of snowflakes that land on your glove in a minute.  Parameter:
    * **lambda** mean number of events.
    
### Continuous

* Uniform
* Normal (gaussian)
* Exponential
* Log-normal --- wait for it ...

Why the "Normal" Distribution is Normal
------------------

* add up uniform
* add up binomial --- these should be binomial if they have the same `prob`, adding them up just increases the `size` parameter.  Prediction: binomial with large `size` will be normal.  Mean is $np$.  Var is $n p (1-p)$.
* Add up two poissons and you should get a poisson.  `lambda` will be the sum of the lambdas.  Mean is lambda.  Variance is lambda.  ACTIVITY: Show that this is true.


Returns on investments
----------------------

Stock market gives return of, say 5%/year  on average with a standard deviation of about 6%.  Simulate the total investment return over 50 years.  

```{r}
prod( 1+rnorm(50,mean=.05,sd=0.06))
```

Have each student do their own, then congratulate the student who got the highest return.

Then show the overall distribution:
```{r}
trials = do(1000)*prod( 1+rnorm(50,mean=.05,sd=0.06))
densityplot(~trials)
```

This distribution has a name: lognormal.  It reflects the fact that the log of the values has a normal distribution.
```{r}
densityplot(~log(trials))
```

Matching a Model to Data
-----------------

In fitting linear models, we've used the sum of square residuals, the difference between the observed and theoretical values squared. $E^2 = \sum (x_i - m_i)^2$

In fitting a probability model (for discrete outcomes) there is a similar approach.

$E^2 = \sum (expected_i - observed_i)^2$

This criterion doesn't really work well.  For instance, if the expected is zero, then the observed should be impossible.  But this formula doesn't reflect that fact.

The standard matching criterion is called a $\chi^2$ (chi-squared) and is
$\chi^2 = \sum \frac{(expected_i - observed_i)^2}{expected^2}$

Another way to measure the match between a set of observations and a probability model is via the **likelihood**: the probability of the observations if the model were right.